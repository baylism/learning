{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Error\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009100000000000002\n"
     ]
    }
   ],
   "source": [
    "o = 0.1\n",
    "d = 8\n",
    "N = 100\n",
    "\n",
    "e_in = (o ** 2)* (1 - ((d + 1) / N))\n",
    "\n",
    "print(e_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Minimise error on nonlinear error surface using gradient descent. Learning rate = 0.1.\n",
    "### Question 4\n",
    "Finding the partial derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2*2.71828182845905**v + 4.0*2.71828182845905**(-u)*v)*(2.71828182845905**v*u - 2*2.71828182845905**(-u)*v)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import mpmath as mp\n",
    "\n",
    "u = sp.Symbol('u')\n",
    "v = sp.Symbol('v')\n",
    "\n",
    "\n",
    "print(sp.diff(((u*mp.e**v)-2*v*mp.e**-u)**2, u))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "How many iterations does it take for the error to fall below 10^-14 for the first time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def in_sample_error(x, y):\n",
    "    \"\"\"Return error from nonlinear error surface E(x, y)\"\"\"\n",
    "    return ((x * (np.e ** y)) - (2 * y * (np.e ** -x))) ** 2\n",
    "\n",
    "\n",
    "def partial_x(x, y):\n",
    "    \"\"\"Return partial derivative E(x, y) with respect to x\"\"\"\n",
    "    return (2*np.e**y + 4.0*np.e**(-x)*y)*(np.e**y*x - 2*np.e**(-x)*y)\n",
    "\n",
    "\n",
    "def partial_y(x, y):\n",
    "    \"\"\"Return partial derivative E(x, y) with respect to y\"\"\"\n",
    "    return (np.e**y*x - 2*np.e**(-x)*y)*(2.0*np.e**y*x - 4*np.e**(-x))\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, learning_rate, target_error):\n",
    "    \"\"\"Run gradient descent according q5 spec\"\"\"\n",
    "    count = 0\n",
    "    error = in_sample_error(x, y)\n",
    "\n",
    "    while error > target_error:\n",
    "        x_temp = x\n",
    "        x = x - learning_rate * partial_x(x, y)\n",
    "        y = y - learning_rate * partial_y(x_temp, y)\n",
    "        error = in_sample_error(x, y)\n",
    "        count += 1\n",
    "\n",
    "    return (count, x, y, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 0.04473629039778207, 0.023958714099141746, 1.2086833944220747e-15)\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent(1, 1, 0.1, 10 ** -14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Coordinate descent'\n",
    "### Question 7\n",
    "In each iteration, we have two steps along the 2 coordinates. Step 1 is to move only along\n",
    "the u coordinate to reduce the error (assume first-order approximation holds like in gradient descent), and step 2 is to reevaluate and move only along the v coordinate to reduce the error (again, assume first-order approximation holds).\n",
    "Use the same learning rate of 0.1 as we did in gradient descent.  \n",
    "  \n",
    "What will the error E(u; v) be closest to after 15 full iterations (30 steps)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coordinate_descent(x, y, learning_rate, iterations):\n",
    "    \"\"\"Run 'coordinate descent' according q7 spec\"\"\"\n",
    "\n",
    "    error = in_sample_error(x, y)\n",
    "\n",
    "    for x in range(iterations):\n",
    "        x = x - learning_rate * partial_x(x, y)\n",
    "        error = in_sample_error(x, y)\n",
    "        y = y - learning_rate * partial_y(x, y)\n",
    "        error = in_sample_error(x, y)\n",
    "\n",
    "    return (x, y, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.998368453740682, -3.7466462092782167, 0.10911280071980022)\n"
     ]
    }
   ],
   "source": [
    "print(coordinate_descent(1, 1, 0.1, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Question 8\n",
    "#### Setting up the experiment\n",
    "In this problem you will create your own target function f (probability in this case) and data set D to see how Logistic Regression works. For simplicity, we will take f to be a 0/1 probability so y is a deterministic function of x.  \n",
    "  \n",
    "Take d = 2 so you can visualize the problem, and let X = [-1,1] X [-1,1] with uniform probability of picking each x in X. Choose a line in the plane as the boundary between f(x) = 1 (where y has to be +1) and f(x) = 0 (where y has to be -1) by taking two random, uniformly distributed points from X and taking the line passing through them as the boundary between y = +-1. Pick N = 100 training points at random from X, and evaluate the outputs yn for each of these points xn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(number_of_points):\n",
    "    \"\"\"Return dataset of random points in form x0=1, x1, x2\"\"\"\n",
    "    ones = np.ones((number_of_points, 1))\n",
    "    points = np.random.uniform(-1.0, 1.0, size=(number_of_points, 2))\n",
    "    return np.concatenate((ones, points), axis=1)\n",
    "\n",
    "\n",
    "def create_f(number_of_points):\n",
    "    \"\"\"Return coeficients of random straight line x0=1, m, c\"\"\"\n",
    "    points = np.random.uniform(-1.0, 1.0, size=(number_of_points, 2))\n",
    "    w0 = 1.0\n",
    "    b = [-w0, -w0]\n",
    "    w1, w2 = np.linalg.solve(points, b)\n",
    "    return np.array([w0, w1, w2])\n",
    "\n",
    "\n",
    "def evaluate_points(dataset, line):\n",
    "    \"\"\"Return list classifying points in dataset as above or below line\"\"\"\n",
    "\n",
    "    return np.sign(dataset.dot(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions for logistic regression with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weights(dataset):\n",
    "    \"\"\"Return empty weight vector of appropriate size for dataset\"\"\"\n",
    "    length = len(dataset[0])\n",
    "    return np.zeros(length, int)\n",
    "\n",
    "\n",
    "def error(point, weights, output):\n",
    "    \"\"\"Return gradient delta Ein for stochastic gradient descent\"\"\"\n",
    "    return (-point * output) / (1 + np.e**(output * weights.dot(point)))\n",
    "\n",
    "\n",
    "def epoch(dataset, output, weights, learning_rate):\n",
    "    \"\"\"Return weights after one epoch\"\"\"\n",
    "    random_order = np.arange(100)\n",
    "    np.random.shuffle(random_order)\n",
    "\n",
    "    for point in random_order:\n",
    "        point_error = error(dataset[point], weights, output[point])\n",
    "        weights = weights - learning_rate * point_error\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def SGD(dataset, outputs, weights, learning_rate, stop):\n",
    "    \"\"\"Return number of epochs and final weights from SGD\"\"\"\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        old_weights = weights\n",
    "        weights = epoch(dataset, outputs, weights, learning_rate)\n",
    "        count += 1\n",
    "        if np.linalg.norm(old_weights - weights) < stop:\n",
    "            break\n",
    "\n",
    "    return (count, weights)\n",
    "\n",
    "\n",
    "def cross_entropy_error(point, weights, output):\n",
    "    return np.log(1 + np.exp(-output * weights.dot(point)))\n",
    "\n",
    "\n",
    "def out_of_sample_error(weights, target_function):\n",
    "    dataset = create_dataset(1000)\n",
    "    outputs = evaluate_points(dataset, target_function)\n",
    "    errors = []\n",
    "    for point in range(len(dataset)):\n",
    "        error = cross_entropy_error(dataset[point], weights, outputs[point])\n",
    "        errors.append(error)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_SGD_experiment(runs):\n",
    "    \"\"\"\n",
    "    Return number of epochs and out of sample error after\n",
    "    runnning SGD according to q8 spec.\n",
    "    \n",
    "    Creates dataset of 100 points, creates line, evaluate points.\n",
    "    \"\"\"\n",
    "\n",
    "    iterations_needed = []\n",
    "    out_of_sample_errors = []\n",
    "\n",
    "    for run in range(runs):\n",
    "        #Status\n",
    "        print(\"Run\" + str(run))\n",
    "        \n",
    "        # Initialisations\n",
    "        dataset = create_dataset(100)\n",
    "        target_function = create_f(2)\n",
    "        outputs = evaluate_points(dataset, target_function)\n",
    "        weights = create_weights(dataset)\n",
    "\n",
    "        # Run SGD\n",
    "        result = SGD(dataset, outputs, weights, 0.01, 0.01)\n",
    "\n",
    "        # Remember number of iterations require to complete SGD for this run\n",
    "        iterations_needed.append(result[0])\n",
    "\n",
    "        # Calculate and remember out of sample error for this run\n",
    "        out_of_sample_errors.append(out_of_sample_error(result[1], target_function))\n",
    "\n",
    "    return (sum(iterations_needed) / runs, np.mean(out_of_sample_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run0\n",
      "Run1\n",
      "Run2\n",
      "Run3\n",
      "Run4\n",
      "Run5\n",
      "Run6\n",
      "Run7\n",
      "Run8\n",
      "Run9\n",
      "(333.5, 0.10064068740898795)\n"
     ]
    }
   ],
   "source": [
    "print(run_SGD_experiment(10))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
