{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "We use the data provided in the files in.dta and out.dta (also used for Homework # 6). Each line of the files corresponds to a two-dimensional input x = (x1, x2), so that X = R^2, followed by the corresponding label from Y = {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = np.loadtxt(\"in.dta\")\n",
    "test = np.loadtxt(\"out.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions for linear regression with nonlinear transformation\n",
    "We apply linear regression with a nonlinear transformation for classifcation (without regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonlinear_transform(data):\n",
    "    \"\"\"Perform nonlinear transformation as per q2 spec\"\"\"\n",
    "    result = []\n",
    "    for row in data:\n",
    "        x1 = row[0]\n",
    "        x2 = row[1]\n",
    "\n",
    "        result.append([1, x1, x2, np.multiply(x1, x1), np.multiply(x2, x2),\n",
    "                       np.multiply(x1, x2), np.abs(x1 - x2), np.abs(x1 + x2)])\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def extract_labels(dataset):\n",
    "    \"\"\"Return correct classifications from dataset\"\"\"\n",
    "    return dataset[:, 2]\n",
    "\n",
    "\n",
    "def linreg(dataset, y):\n",
    "    \"\"\"Return weights from linear regression\"\"\"\n",
    "    pseudo_inverse = np.linalg.pinv(dataset)\n",
    "    weights = pseudo_inverse.dot(y)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def linreg_weight_decay(dataset, y, reg_factor):\n",
    "    \"\"\"Return weights from linear regression with weight decay\"\"\"\n",
    "    a = dataset.T.dot(dataset) + (np.identity(dataset.shape[1]) * reg_factor)\n",
    "    b = np.linalg.inv(a)\n",
    "    c = b.dot(dataset.T)\n",
    "    weights = c.dot(y)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def evaluate_points(dataset, line):\n",
    "    \"\"\"Return list classifying points in dataset as above or below line\"\"\"\n",
    "\n",
    "    return np.sign(dataset.dot(line))\n",
    "\n",
    "\n",
    "def calculate_error(dataset, weights, y):\n",
    "    \"\"\"Calculate error in weights\"\"\"\n",
    "    output = evaluate_points(dataset, weights)\n",
    "    comparison = np.equal(output, y)\n",
    "\n",
    "    number_false = 0.0\n",
    "    for c in comparison:\n",
    "        if c == False:\n",
    "            number_false += 1\n",
    "\n",
    "    return number_false / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = train[:25]\n",
    "validation = train[25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Using validation set to select between five models that apply linear regression to the nonlinear transformation 0 through k, where k is the number of transformations.  \n",
    "  \n",
    "First, we apply whole nonlinear transform to training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_labels = extract_labels(training)\n",
    "training_transformed = nonlinear_transform(training)\n",
    "\n",
    "validation_labels = extract_labels(validation)\n",
    "validation_transformed = nonlinear_transform(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then apply linear regression to the relevant part of the transformed data for each k and record the validation error for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation errors:  {3: 0.3, 4: 0.5, 5: 0.2, 6: 0.0, 7: 0.1}\n"
     ]
    }
   ],
   "source": [
    "validation_errors = {}\n",
    "for k in [3, 4, 5, 6, 7]:\n",
    "   training_data = training_transformed[:, :k+1]\n",
    "   validation_data = validation_transformed[:, :k+1]\n",
    "\n",
    "   regression_weights = linreg(training_data, training_labels)\n",
    "   validation_errors[k] = calculate_error(validation_data, regression_weights,\n",
    "                                          validation_labels)\n",
    "\n",
    "print(\"Validation errors: \", validation_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification error on the validation set is smallest for k = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Evaluating the out of sample classifiation error on the 5 models from Question 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation errors:  {3: 0.3, 4: 0.5, 5: 0.2, 6: 0.0, 7: 0.1}\n",
      "Out of sample errors:  {3: 0.42, 4: 0.416, 5: 0.188, 6: 0.084, 7: 0.072}\n"
     ]
    }
   ],
   "source": [
    "training_labels = extract_labels(training)\n",
    "training_transformed = nonlinear_transform(training)\n",
    "\n",
    "validation_labels = extract_labels(validation)\n",
    "validation_transformed = nonlinear_transform(validation)\n",
    "\n",
    "test_labels = extract_labels(test)\n",
    "test_transformed = nonlinear_transform(test)\n",
    "\n",
    "validation_errors = {}\n",
    "out_sample_errors = {}\n",
    "\n",
    "for k in [3, 4, 5, 6, 7]:\n",
    "    training_data = training_transformed[:, :k+1]\n",
    "    validation_data = validation_transformed[:, :k+1]\n",
    "    test_data = test_transformed[:, :k+1]\n",
    "\n",
    "    regression_weights = linreg(training_data, training_labels)\n",
    "\n",
    "    validation_errors[k] = calculate_error(validation_data, regression_weights,\n",
    "                                           validation_labels)\n",
    "    out_sample_errors[k] = calculate_error(test_data, regression_weights,\n",
    "                                           test_labels)\n",
    "\n",
    "print(\"Validation errors: \", validation_errors)\n",
    "print(\"Out of sample errors: \", out_sample_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Reversing the role of training and validation sets. Train with last 10 examples and validate on first 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation errors:  {3: 0.28, 4: 0.36, 5: 0.2, 6: 0.08, 7: 0.12}\n"
     ]
    }
   ],
   "source": [
    "training_labels = extract_labels(validation)\n",
    "training_transformed = nonlinear_transform(validation)\n",
    "\n",
    "validation_labels = extract_labels(training)\n",
    "validation_transformed = nonlinear_transform(training)\n",
    "\n",
    "validation_errors = {}\n",
    "\n",
    "for k in [3, 4, 5, 6, 7]:\n",
    "   training_data = training_transformed[:, :k+1]\n",
    "   validation_data = validation_transformed[:, :k+1]\n",
    "\n",
    "   regression_weights = linreg(training_data, training_labels)\n",
    "\n",
    "   validation_errors[k] = calculate_error(validation_data, regression_weights,\n",
    "                                          validation_labels)\n",
    "\n",
    "print(\"Validation errors: \", validation_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Evaluating the out of sample classifiation error on the 5 models from Question 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation errors:  {3: 0.28, 4: 0.36, 5: 0.2, 6: 0.08, 7: 0.12}\n",
      "Out of sample errors:  {3: 0.396, 4: 0.388, 5: 0.284, 6: 0.192, 7: 0.196}\n"
     ]
    }
   ],
   "source": [
    "training_labels = extract_labels(validation)\n",
    "training_transformed = nonlinear_transform(validation)\n",
    "\n",
    "validation_labels = extract_labels(training)\n",
    "validation_transformed = nonlinear_transform(training)\n",
    "\n",
    "test_labels = extract_labels(test)\n",
    "test_transformed = nonlinear_transform(test)\n",
    "\n",
    "validation_errors = {}\n",
    "out_sample_errors = {}\n",
    "\n",
    "for k in [3, 4, 5, 6, 7]:\n",
    "   training_data = training_transformed[:, :k+1]\n",
    "   validation_data = validation_transformed[:, :k+1]\n",
    "   test_data = test_transformed[:, :k+1]\n",
    "\n",
    "   regression_weights = linreg(training_data, training_labels)\n",
    "\n",
    "   validation_errors[k] = calculate_error(validation_data, regression_weights,\n",
    "                                          validation_labels)\n",
    "   out_sample_errors[k] = calculate_error(test_data, regression_weights,\n",
    "                                          test_labels)\n",
    "\n",
    "print(\"Validation errors: \", validation_errors)\n",
    "print(\"Out of sample errors: \", out_sample_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the out of sample classification errors for the models chosen in Questions 1 and 3.\n",
    "\n",
    "Eout for k=6 in Question 3 = 0.192  \n",
    "Eout for k=6 in Question 1 = 0.084  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### Validation bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let e1 and e2 be independent random variables, distributed uniformly over the\n",
    "interval [0, 1]. Let e = min(e1, e2). The expected values of e1, e2, e are closest to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 100000\n",
      "Expected e1 = 0.5011582027679854\n",
      "Expected e2 = 0.4990039527090463\n",
      "Expected min = 0.3339748374582504\n"
     ]
    }
   ],
   "source": [
    "def validation_bias(runs):\n",
    "    data = np.zeros((runs, 3))\n",
    "\n",
    "    for row in data:\n",
    "        row[0] = np.random.uniform(0.0, 1.0)\n",
    "        row[1] = np.random.uniform(0.0, 1.0)\n",
    "        row[2] = min(row[1], row[0])\n",
    "\n",
    "    return data\n",
    "\n",
    "runs = 100000\n",
    "data = validation_bias(runs)\n",
    "\n",
    "expected_e1 = np.mean(data[:, 0])\n",
    "expected_e2 = np.mean(data[:, 1])\n",
    "expected_min = np.mean(data[:, 2])\n",
    "\n",
    "print(\"Runs: {0}\".format(runs))\n",
    "print(\"Expected e1 = {0}\".format(expected_e1))\n",
    "print(\"Expected e2 = {0}\".format(expected_e2))\n",
    "print(\"Expected min = {0}\".format(expected_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "#### PLA vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import quadprog as qp\n",
    "\n",
    "\n",
    "def create_dataset(number_of_points):\n",
    "    \"\"\"Return dataset of random points in form x0=1, x1, x2\"\"\"\n",
    "    ones = np.ones((number_of_points, 1))\n",
    "    points = np.random.uniform(-1.0, 1.0, size=(number_of_points, 2))\n",
    "    return np.concatenate((ones, points), axis=1)\n",
    "\n",
    "\n",
    "def create_f(points):\n",
    "    \"\"\"Return coeficients of random straight line x0=1, m, c\"\"\"\n",
    "    points = np.random.uniform(-1.0, 1.0, size=(points, 2))\n",
    "    p0 = 1.0\n",
    "    b = [-p0, -p0]\n",
    "    w1, w2 = np.linalg.solve(points, b)\n",
    "    return np.array([p0, w1, w2])\n",
    "\n",
    "\n",
    "def evaluate_points(dataset, line):\n",
    "    \"\"\"Return list classifying points in dataset as above or below line\"\"\"\n",
    "    return np.sign(dataset.dot(line))\n",
    "\n",
    "\n",
    "def create_weights(dataset):\n",
    "    \"\"\"Return empty weight vector of appropriate size for dataset\"\"\"\n",
    "    length = len(dataset[0])\n",
    "    return np.zeros(length, int)\n",
    "\n",
    "\n",
    "def check_classifications(dataset, weights, y):\n",
    "    \"\"\"Return list of misclassified points in dataset\"\"\"\n",
    "    misclassified_points = []\n",
    "\n",
    "    for point_index in range(len(dataset)):\n",
    "        if np.sign(dataset[point_index].dot(weights)) != y[point_index]:\n",
    "            misclassified_points.append(point_index)\n",
    "\n",
    "    return misclassified_points\n",
    "\n",
    "\n",
    "def nudge(dataset, y, weights, misclassified_points):\n",
    "    \"\"\"Update weights using a random misclassified point\"\"\"\n",
    "    point_index = np.random.choice(misclassified_points)\n",
    "    weights = weights + y[point_index] * dataset[point_index]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def compare_weights(weights_1, weights_2, runs):\n",
    "    test_points = create_dataset(runs)\n",
    "    labels_1 = evaluate_points(test_points, weights_1)\n",
    "    labels_2 = evaluate_points(test_points, weights_2)\n",
    "    print(\"l1: \" + str(len(labels_1)))\n",
    "    print(\"l2: \" + str(len(labels_2)))\n",
    "\n",
    "    differences = 0\n",
    "    for point in range(runs):\n",
    "        if labels_1[point] == labels_2[point]:\n",
    "            differences += 1\n",
    "\n",
    "    return differences / runs\n",
    "\n",
    "\n",
    "def run_perceptron(number_of_points):\n",
    "    \"\"\"Return weights from PLA after all points classified correctly\"\"\"\n",
    "\n",
    "    # Ensure all points not on same side of line\n",
    "\n",
    "    while True:\n",
    "        dataset = create_dataset(number_of_points)\n",
    "        target_function = create_f(2)\n",
    "        labels = evaluate_points(dataset, target_function)\n",
    "        if not np.all(labels == labels[0]):\n",
    "            break\n",
    "\n",
    "    weights = create_weights(dataset)\n",
    "\n",
    "    while True:\n",
    "        misclassified_points = check_classifications(dataset, weights, labels)\n",
    "        if misclassified_points:\n",
    "            weights = nudge(dataset, labels, weights, misclassified_points)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return compare_weights(weights, target_function, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Buffer has wrong number of dimensions (expected 1, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f740d11183a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mtarget_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Run Perceptron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-f740d11183a8>\u001b[0m in \u001b[0;36mSVM\u001b[1;34m(dataset, y)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mqp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve_qp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mquadprog/quadprog.pyx\u001b[0m in \u001b[0;36mquadprog.solve_qp (quadprog/quadprog.cpp:1484)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 2)"
     ]
    }
   ],
   "source": [
    "def create_G(dataset, y):\n",
    "    points = dataset[:, 1:]\n",
    "    G = np.zeros((points.shape[0], points.shape[0]))\n",
    "    for row in range(points.shape[0]):\n",
    "        for col in range(points.shape[0]):\n",
    "            val = (y[row] * y[col]) * points[row].dot(points[col])\n",
    "\n",
    "            G[row][col] = val\n",
    "\n",
    "    return G\n",
    "\n",
    "def create_a(N):\n",
    "    return np.full((N, 1), -1.)\n",
    "\n",
    "def create_C(y, N):\n",
    "    return np.hstack((-y.reshape((N, 1)), np.identity(N)))\n",
    "\n",
    "def create_b(N):\n",
    "    return np.full((N, 1), 0.)\n",
    "\n",
    "def SVM(dataset, y):\n",
    "    N = dataset.shape[0]\n",
    "    G = create_G(dataset, y)\n",
    "    a = create_a(N)\n",
    "    C = create_C(y, N)\n",
    "    b = create_b(N)\n",
    "\n",
    "    return qp.solve_qp(G, a, C, b, meq=1)\n",
    "\n",
    "\n",
    "# Run SVM\n",
    "dataset = create_dataset(10)\n",
    "target_function = create_f(2)\n",
    "labels = evaluate_points(dataset, target_function)\n",
    "result = SVM(dataset, labels)\n",
    "\n",
    "# Run Perceptron\n",
    "x = run_perceptron(1000)\n",
    "\n",
    "print(result)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
